{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Mode Recognition in Multi-Cultural Context\n",
    "\n",
    "## STEP 1 : Experiments on OTMM Dataset\n",
    "\n",
    " - Set your Dunya token to have access for Dunya datasets.\n",
    " \n",
    " - Set \n",
    " \n",
    "       EXTRACT_FEATURES = True\n",
    "   for extracting features for the first time.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import modeUtils.Containers as C\n",
    "\n",
    "DUNYA_TOKEN = '___Enter Your Dunya User Token Here____'\n",
    "\n",
    "# If you would like to skip feature extraction and perform classification \n",
    "# with default parameters, set ' EXTRACT_FEATURES = False '. \n",
    "EXTRACT_FEATURES = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set analysis parameters\n",
    "\n",
    " - AnalysisParams : class type object for analysis parameters\n",
    " \n",
    " - You can :\n",
    " \n",
    "     - change the windowing parameters\n",
    " \n",
    "     - change the chroma resolution using 'chroma_NumBins'\n",
    "      \n",
    "     - choose which features to use (local or global)\n",
    "     \n",
    "     - adjust the part of song to extract features (0 is beginning and 1 is full song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WindowSize = 200 #ms\n",
    "HopSize = 100 #ms\n",
    "WindowFunction = 'hann' # 'Hanning'\n",
    "fftN = 2048\n",
    "sampleRate = 44100\n",
    "chroma_NumBins = 12\n",
    "Features_Region = ['local','full']\n",
    "LocalRegion = 0.3 # 30% from the beginning of the whole song\n",
    "parameters = C.AnalysisParams(WindowSize,\n",
    "                              HopSize,\n",
    "                              WindowFunction,\n",
    "                              fftN,\n",
    "                              sampleRate,\n",
    "                              chroma_NumBins,\n",
    "                              Features_Region,\n",
    "                              LocalRegion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset\n",
    "\n",
    " - Initiate Collection class type object for Ottoman Turkish Makam Music Dataset and set parameters for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = C.Collection()\n",
    "dataset.set_dunya_token(DUNYA_TOKEN)\n",
    "dataset.set_parameters(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Download dataset using Dunya API to the directory below.\n",
    " - Use \n",
    " \n",
    "       Collection.download_collection('annotations.json') \n",
    "   method automatically downloads the songs in the annotation file and parses related information for further processing into Recording class type objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and created in data/Turkish/ folder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotationFile = 'data/Turkish/annotations_Turkish.json'\n",
    "dataset.download_collection(annotationFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - To see the attributes of each recording, use \n",
    " \n",
    "       Collection.get_recording('song_MBID')\n",
    "   method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = dataset.get_recording('4b31c6ab-c401-42d7-af85-a330aef806d3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "   - Features are extracted and saved within Container object. The Recording features and metadata are stored in Container.recordings attribute, which is a dictionary of Recording objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXTRACT_FEATURES:\n",
    "    dataset.extract_chroma_features()\n",
    "else:\n",
    "    FeaturesData = os.path.join(dataset.path, \n",
    "                                'extractedFeatures_for' + dataset.mode + \n",
    "                                'tradition(' + str(dataset.analysis_parameters.numbins) + 'bins).pkl') \n",
    "    dataset.load_features(FeaturesData)\n",
    "    \n",
    "if 'local' in Features_Region:\n",
    "    dataset.extract_local_chroma_features()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "  - Manually select which chunk of features to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features4Classification = ['chroma_std','chroma_std_local']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Classification \n",
    " - Initiate SupervisedLearning object for performing the automatic classification.\n",
    " - Create a .csv files containing features for classification for the purpose of rapid experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModeRecognition = C.SupervisedLearning()\n",
    "ModeRecognition.feature_selection(Features4Classification)\n",
    "ModeRecognition.create_dataframe(dataset)\n",
    "ModeRecognition.read_dataframe(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Apply automatic classification with Support Vector Machines as classifiers.\n",
    " - The classifier model parameters are optimized using Grid Search Method, which are validated using 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ModeRecognition.evaluate_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Print out the standard evalution scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ModeRecognition.scores_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ModeRecognition.scores_fMeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 : Mode Recognition on Single Audio File\n",
    "\n",
    " Procedure : \n",
    " \n",
    "  - Train the classifier with the whole dataset.\n",
    "  \n",
    "  - Extract features of the audio file for mode recognition.\n",
    "  \n",
    "  - Predict the mode type using the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModeRecognition.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initiate Recording object for the song to estimate mode type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewSong = C.Recording()\n",
    "NewSong.load_recording('0f0e4bc3-67f9-4727-818b-983320e897cb.mp3',parameters)\n",
    "\n",
    "DetectTonicLastNote(NewSong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Predict the mode type using the classifier that is trained on the Dunya dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedModeType = ModeRecognition.predict_mode_recording(NewSong,parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print out the predicted Mode Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Saba'], dtype=object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedModeType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225.79343245589331"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NewSong.tonic "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
