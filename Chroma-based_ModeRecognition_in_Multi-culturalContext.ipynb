{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUDIO CLASSIFICATION BASED ON 'MODALITY' USING CHROMA FEATURES\n",
    "\n",
    "\n",
    "#### This series of notebooks demonstrate a tutorial for classifying audio files based on the tonal concept of 'modality in music'. The concept of 'modality' exists in various tonal music traditions around the world. In this tutorial, we apply supervised learning based on chroma features.\n",
    "\n",
    "There are 2 main steps for the audio classification: Feature Extraction and Classification. \n",
    "---\n",
    "#### This notebook shows the steps of Chroma Feature Based Automatic Makam Recognition.\n",
    "\n",
    "\n",
    "#### PARAMETERS:\n",
    " - numBins(int) : Number of bins in the Chroma Vectors. This is parametrized in consideration of possible microtonalities existing in non-Western music traditions (12,24,36,48, ...)\n",
    " - modality(str) : Name of the modality type specific for the music tradition.\n",
    " \n",
    " \n",
    "         Classical Western Music / Jazz : Mode, tonality, scale\n",
    " \n",
    "         Classical Turkish Music : Makam\n",
    " \n",
    "         Classical North Indian Music : Rag, Raga\n",
    " \n",
    "         Classical Arab-Andalusian Music : Tab\n",
    " \n",
    "\n",
    "IMPORTANT : All the audio files should be located in the directory specifically according to their modality type.\n",
    "\n",
    "Note that the functions that are used in this tutorial can be found in ######(TODO) that is provided in the repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 : GATHERING DATA / DOWNLOADING THE AUDIO FILES\n",
    "\n",
    "### Input : \n",
    "#### JSON file :\n",
    "which contains the mbids of audio files, the tonic information, the modality category (e.g. makam, mode, rag, tab, etc. ) In the case of multiple modalities within one song, the start and end times of different sections needs to be specified.\n",
    "\n",
    "#### Directory :\n",
    "Output directory for the dataset to be downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compmusic import dunya\n",
    "from modalityUtils.utilities import downloadDataset\n",
    "\n",
    "#token will be shared upon request\n",
    "dunya.set_token('___yourTOKENhere___') \n",
    "\n",
    "### Please set the directory for the dataset to be downloaded and the name of the annotations file (JSON).\n",
    "dataDir = 'data/'\n",
    "annotationsFile = 'annotations.json'\n",
    "modality = 'makam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadDataset(annotationsFile,dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 : FEATURE EXTRACTION\n",
    "\n",
    "### Input : \n",
    " - JSON file :\n",
    "    which contains the mbids of audio files, the tonic information, the modality category (e.g. makam, mode, rag, tab, etc. ) In the case of multiple modalities within one song, the start and end times of different sections needs to be specified.\n",
    "\n",
    " - numBins(int) : Number of bins in the Chroma Vectors. This is parametrized in consideration of possible microtonalities existing in non-Western music traditions (12,24,36,48, ...)\n",
    " - musicTradition (str) : Name of the modality type specific for the music tradition.\n",
    " \n",
    " \n",
    "         Classical Western Music / Jazz : Mode, tonality, scale\n",
    " \n",
    "         Classical Turkish Music : Makam\n",
    " \n",
    "         Classical North Indian Music : Rag, Raga\n",
    " \n",
    "         Classical Arab-Andalusian Music : Tab\n",
    " \n",
    "\n",
    "### Output : \n",
    "#### PICKLE file :\n",
    "which has a list of dictionaries where each dictionary contains the Frame-based features, the global statistical features and the ground truth information (tonic, modality type) of each of the audio files.\n",
    "\n",
    "#### CSV file : \n",
    "which has the global statistical features of audio files and the ground truth modality type. This CSV file is generated in the proper format for further Machine Learning / Automatic Classification steps. Use this CSV file as the input for the second notebook.\n",
    "\n",
    "\n",
    "IMPORTANT : All the audio files should be located in the directory specifically according to their modality type.\n",
    "\n",
    "\n",
    "Note that the functions that are used in this tutorial can be found in modalityUtils/utilities.py, which is provided in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ChromaFeatureExtraction.py [-h] -t TRADITION -n NUMBEROFBINS -o\r\n",
      "                                  OUTPUT_DIRECTORY\r\n",
      "\r\n",
      "A tool for Chroma (HPCP) Feature Extraction using Essentia library.\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -t TRADITION, --tradition TRADITION\r\n",
      "                        Input music tradition to perform the modality\r\n",
      "                        classification task\r\n",
      "  -n NUMBEROFBINS, --numberofBins NUMBEROFBINS\r\n",
      "                        Input number of bins per octave in chroma vectors\r\n",
      "  -o OUTPUT_DIRECTORY, --output_directory OUTPUT_DIRECTORY\r\n",
      "                        Output directory for the pickle file that contains the\r\n",
      "                        dataset with the extracted features\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ChromaFeatureExtraction.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis on TurkishClassicalMusic Tradition.\n",
      "\n",
      "Number of bins per octave in the Chroma Vectors : 12\n",
      "Modality categories in the dataset : \n",
      "\n",
      "{'Kurdilihicazkar', 'Mahur', 'Muhayyer', 'Sultaniyegah', 'Suzinak', 'Ussak', 'Huseyni', 'Beyati', 'Rast', 'Karcigar', 'Nihavent', 'Hicazkar', 'Bestenigar', 'Acemkurdi', 'Hicaz', 'Neva', 'Acemasiran', 'Saba', 'Segah', 'Huzzam'} \n",
      "\n",
      "Number of Categories in the dataset\n",
      "20\n",
      "Feature Extraction in Process. This might take a while...\n",
      "extracting Features for modality :  Acemasiran\n",
      "extracting Features for modality :  Acemkurdi\n",
      "extracting Features for modality :  Bestenigar\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"ChromaFeatureExtraction.py\", line 63, in <module>\n",
      "    main(MusicTradition, numBins, outputDir)\n",
      "  File \"ChromaFeatureExtraction.py\", line 57, in main\n",
      "    dataslist = mu.FeatureExtraction(outputDir,dataDir,dataList,modality)\n",
      "  File \"/home/ds/notebooks/Supervised_Modality_Classification/modalityUtils.py\", line 205, in FeatureExtraction\n",
      "    computeHPCPFeatures(dataList[ind],params,numBins,modality)\n",
      "  File \"/home/ds/notebooks/Supervised_Modality_Classification/modalityUtils.py\", line 155, in computeHPCPFeatures\n",
      "    HPCPs = computeHPCP(x, windowSize, hopSize, params, tonic, numBin)\n",
      "  File \"/home/ds/notebooks/Supervised_Modality_Classification/modalityUtils.py\", line 129, in computeHPCP\n",
      "    mX = ess.Spectrum(size=windowSize)(frame)\n",
      "  File \"/usr/local/lib/python3/dist-packages/essentia/standard.py\", line 44, in __init__\n",
      "    self.configure(**kwargs)\n",
      "  File \"/usr/local/lib/python3/dist-packages/essentia/standard.py\", line 64, in configure\n",
      "    self.__configure__(**kwargs)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 ChromaFeatureExtraction.py -t TurkishClassicalMusic -n 12 -o outDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: DataFormatting.py [-h] -i INPUT_FILENAME -f FEATURES_SET -r REGION -c\r\n",
      "                         COMBINED\r\n",
      "\r\n",
      "A tool for converting the feature data into proper format for the machine\r\n",
      "learning pipeline\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -i INPUT_FILENAME, --input_filename INPUT_FILENAME\r\n",
      "                        Name of the input pickle file which is the output file\r\n",
      "                        of ChromaFeatureExtraction.py\r\n",
      "  -f FEATURES_SET, --features_set FEATURES_SET\r\n",
      "                        Set of features to include in the Feature Data.\r\n",
      "                        options = {\"mean\",\"std\",\"all\"}\r\n",
      "  -r REGION, --region REGION\r\n",
      "                        Input region of audios (from the beginning) for\r\n",
      "                        extracting local features. values = [0,1] (region = 1\r\n",
      "                        is equivalent of using the Global Features)\r\n",
      "  -c COMBINED, --combined COMBINED\r\n",
      "                        Input number of bins per octave in chroma vectors SET\r\n",
      "                        combined = 1 FOR PERFORMING CLASSIFICATION USING THE\r\n",
      "                        COMBINATION OF LOCAL AND GLOBAL FEATURES; combined = 0\r\n",
      "                        FOR USING ONLY THE LOCAL FEATURES (except the case\r\n",
      "                        region = 1, which ALREADY corresponds to global\r\n",
      "                        features)\r\n"
     ]
    }
   ],
   "source": [
    "!python3 DataFormatting.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 :  MACHINE LEARNING & AUTOMATIC AUDIO CLASSIFICATION\n",
    "\n",
    "### Input : \n",
    "#### PICKLE file : \n",
    "which is the output of the first notebook in the series (Feature Extraction).\n",
    "\n",
    "\n",
    "### Output :\n",
    "#### CSV file : \n",
    "The output CSV file that contains the FeatureData (X) and the class labels (Y), in appropriate format for the Machine Learning Pipeline.\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "\n",
    " - numBins(int) : number of bins in the HPCPvectors ([12,24,36,48])\n",
    " \n",
    " \n",
    " - region(float) : select the first 'x*100'% of the audio files for analysis. The classification is performed using the features that are extracted locally only from specified region of the songs.\n",
    " \n",
    "\n",
    " - combined (boolean) : if combined = 1, classification is performed using the combination of features that are extracted both locally and globally. if combined = 0, only locally obtained features are used for classification. The local region is specified by partSong. Default = 1.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing Local Features for the first0.3region of the audio files is COMPLETE \n",
      "\n",
      "Generating CSV file for the features meanLocal+stdGlobal is COMPLETE\n",
      "Generating CSV file for the features stdLocal+meanGlobal is COMPLETE\n",
      "Generating CSV file for the features meanLocal+meanGlobal is COMPLETE\n",
      "Generating CSV file for the features stdLocal+stdGlobal is COMPLETE\n"
     ]
    }
   ],
   "source": [
    "!python3 DataFormatting.py -i 'extractedFeatures_formakamtradition(48bins).pkl' -r 0.3 -c False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: Classification.py [-h] -i INPUT_FILENAME -c COMBINED -r REGION\r\n",
      "\r\n",
      "A tool for Chroma (HPCP) Feature Extraction using Essentia\r\n",
      "library.(CLASSIFICATION)\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -i INPUT_FILENAME, --input_filename INPUT_FILENAME\r\n",
      "                        Name of the input pickle file which is the output file\r\n",
      "                        of ChromaFeatureExtraction.py\r\n",
      "  -c COMBINED, --combined COMBINED\r\n",
      "                        Input number of bins per octave in chroma vectors SET\r\n",
      "                        combined = 1 FOR PERFORMING CLASSIFICATION USING THE\r\n",
      "                        COMBINATION OF LOCAL AND GLOBAL FEATURES; combined = 0\r\n",
      "                        FOR USING ONLY THE LOCAL FEATURES (except the case\r\n",
      "                        region = 1, which ALREADY corresponds to global\r\n",
      "                        features)\r\n",
      "  -r REGION, --region REGION\r\n",
      "                        Input region of audios (from the beginning) for\r\n",
      "                        extracting local features , SELECT THE REGION OF THE\r\n",
      "                        SONG TO BE ANALYZED LOCALLY. (region = 1 is equivalent\r\n",
      "                        of using the Global Features)\r\n"
     ]
    }
   ],
   "source": [
    "!python3  Classification.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Huzzam',\n",
       " 'annotations.json',\n",
       " 'DataCSVforstage_48bins_meanLocal+stdGlobal.csv',\n",
       " 'DataCSVforstage_48bins_meanLocal+meanGlobal.csv',\n",
       " 'extractedFeatures_formakamtradition(48bins).pkl',\n",
       " 'scoresstdLocal+stdGlobal_48.txt',\n",
       " 'scoresmeanLocal+stdGlobal_48.txt',\n",
       " 'DataCSVforstage_48bins_stdLocal+meanGlobal.csv',\n",
       " 'DataCSVforstage_48bins_stdLocal+stdGlobal.csv',\n",
       " 'scoresstdLocal+meanGlobal_48.txt']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###CHOOSE THE CSV FILE WITH DESIRED FEATURE SET, TO INPUT FOR THE MACHINE LEARNING PIPELINE\n",
    "import os\n",
    "os.listdir(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process might take a while (5-10 min) \n",
      " CROSS-VALIDATION & TRAINING \n",
      "{'Ussak', 'Rast', 'Nihavent', 'Beyati', 'Karcigar', 'Suzinak', 'Neva', 'Huseyni', 'Saba', 'Acemasiran', 'Hicaz', 'Kurdilihicazkar', 'Muhayyer', 'Acemkurdi', 'Sultaniyegah', 'Mahur', 'Hicazkar', 'Huzzam', 'Bestenigar', 'Segah'}\n",
      "Accuracy score for the Feature Set stdLocal+stdGlobal : \n",
      "F-measure (mean,std) --- FINAL\n",
      "0.76 0.0348500070129\n",
      "Accuracy (mean,std) FINAL\n",
      "0.77 0.0366196668472\n",
      "Confusion matrix, without normalization\n"
     ]
    }
   ],
   "source": [
    "!python3 Classification.py -i 'DataCSVforstage_48bins_stdLocal+stdGlobal.csv' -m makam -r 0.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
