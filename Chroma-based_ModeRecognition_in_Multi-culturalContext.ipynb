{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHROMA-BASED SUPERVISED MODE RECOGNITION IN THE MULTI-CULTURAL CONTEXT\n",
    "\n",
    "\n",
    "There are 3 main steps in the workflow of this notebook: Feature Extraction, Data Formatting and Classification. \n",
    "---\n",
    "\n",
    "IMPORTANT : All the audio files should be located in the directory specifically according to their modality type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 : GATHERING DATA / DOWNLOADING THE AUDIO FILES\n",
    "\n",
    "### Input : \n",
    "#### JSON file :\n",
    "which contains the mbids of audio files, the tonic information, the modality category (e.g. makam, mode, rag, tab, etc. ) In the case of multiple modalities within one song, the start and end times of different sections needs to be specified.\n",
    "\n",
    "#### Directory :\n",
    "Output directory for the dataset to be downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'modalityUtils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1ed03f5eadb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcompmusic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdunya\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodalityUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownloadDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#token will be shared upon request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdunya\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'___yourTOKENhere___'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'modalityUtils'"
     ]
    }
   ],
   "source": [
    "from compmusic import dunya\n",
    "from modalityUtils.utilities import downloadDataset\n",
    "\n",
    "#token will be shared upon request\n",
    "dunya.set_token('___yourTOKENhere___') \n",
    "\n",
    "### Please set the directory for the dataset to be downloaded and the name of the annotations file (JSON).\n",
    "dataDir = 'data/'\n",
    "annotationsFile = 'annotations.json'\n",
    "modality = 'makam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadDataset(annotationsFile,dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 : FEATURE EXTRACTION\n",
    "\n",
    "### Input : \n",
    " - JSON file :\n",
    "    which contains the mbids of audio files, the tonic information, the modality category (e.g. makam, mode, rag, tab, etc. ) In the case of multiple modalities within one song, the start and end times of different sections needs to be specified.\n",
    "\n",
    " - numBins(int) : Number of bins in the Chroma Vectors. This is parametrized in consideration of possible microtonalities existing in non-Western music traditions (12,24,36,48, ...)\n",
    " - musicTradition (str) : Name of the modality type specific for the music tradition.\n",
    " \n",
    " \n",
    "         Jazz : Chord-scale\n",
    " \n",
    "         Turkish Classical Music : Makam\n",
    " \n",
    "         Hindustani Classical Music : Rag\n",
    " \n",
    "         Carnatic Classical Music : Raaga\n",
    " \n",
    "         Arab-Andalusian Music : Tab\n",
    "\n",
    "### Output : \n",
    "#### PICKLE file :\n",
    "which has a list of dictionaries where each dictionary contains the Frame-based features, the global statistical features and the ground truth information (tonic, modality type) of each of the audio files.\n",
    "\n",
    "##### IMPORTANT : All the audio files should be located in the directory specifically according to their modality type.\n",
    "\n",
    "\n",
    "Note that the functions that are used in this tutorial can be found in modalityUtils/utilities.py, which is provided in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ChromaFeatureExtraction.py [-h] -t TRADITION -n NUMBEROFBINS -o\r\n",
      "                                  OUTPUT_DIRECTORY\r\n",
      "\r\n",
      "A tool for Chroma (HPCP) Feature Extraction using Essentia library.\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -t TRADITION, --tradition TRADITION\r\n",
      "                        Input music tradition to perform the modality\r\n",
      "                        classification task\r\n",
      "  -n NUMBEROFBINS, --numberofBins NUMBEROFBINS\r\n",
      "                        Input number of bins per octave in chroma vectors\r\n",
      "  -o OUTPUT_DIRECTORY, --output_directory OUTPUT_DIRECTORY\r\n",
      "                        Output directory for the pickle file that contains the\r\n",
      "                        dataset with the extracted features\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ChromaFeatureExtraction.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis on TurkishClassicalMusic Tradition.\n",
      "\n",
      "Number of bins per octave in the Chroma Vectors : 12\n",
      "Modality categories in the dataset : \n",
      "\n",
      "{'Kurdilihicazkar', 'Mahur', 'Muhayyer', 'Sultaniyegah', 'Suzinak', 'Ussak', 'Huseyni', 'Beyati', 'Rast', 'Karcigar', 'Nihavent', 'Hicazkar', 'Bestenigar', 'Acemkurdi', 'Hicaz', 'Neva', 'Acemasiran', 'Saba', 'Segah', 'Huzzam'} \n",
      "\n",
      "Number of Categories in the dataset\n",
      "20\n",
      "Feature Extraction in Process. This might take a while...\n",
      "extracting Features for modality :  Acemasiran\n",
      "extracting Features for modality :  Acemkurdi\n",
      "extracting Features for modality :  Bestenigar\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"ChromaFeatureExtraction.py\", line 63, in <module>\n",
      "    main(MusicTradition, numBins, outputDir)\n",
      "  File \"ChromaFeatureExtraction.py\", line 57, in main\n",
      "    dataslist = mu.FeatureExtraction(outputDir,dataDir,dataList,modality)\n",
      "  File \"/home/ds/notebooks/Supervised_Modality_Classification/modalityUtils.py\", line 205, in FeatureExtraction\n",
      "    computeHPCPFeatures(dataList[ind],params,numBins,modality)\n",
      "  File \"/home/ds/notebooks/Supervised_Modality_Classification/modalityUtils.py\", line 155, in computeHPCPFeatures\n",
      "    HPCPs = computeHPCP(x, windowSize, hopSize, params, tonic, numBin)\n",
      "  File \"/home/ds/notebooks/Supervised_Modality_Classification/modalityUtils.py\", line 129, in computeHPCP\n",
      "    mX = ess.Spectrum(size=windowSize)(frame)\n",
      "  File \"/usr/local/lib/python3/dist-packages/essentia/standard.py\", line 44, in __init__\n",
      "    self.configure(**kwargs)\n",
      "  File \"/usr/local/lib/python3/dist-packages/essentia/standard.py\", line 64, in configure\n",
      "    self.__configure__(**kwargs)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 ChromaFeatureExtraction.py -t TurkishClassicalMusic -n 12 -o outDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: DataFormatting.py [-h] -i INPUT_FILENAME -f FEATURES_SET -r REGION -c\r\n",
      "                         COMBINED\r\n",
      "\r\n",
      "A tool for converting the feature data into proper format for the machine\r\n",
      "learning pipeline\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -i INPUT_FILENAME, --input_filename INPUT_FILENAME\r\n",
      "                        Name of the input pickle file which is the output file\r\n",
      "                        of ChromaFeatureExtraction.py\r\n",
      "  -f FEATURES_SET, --features_set FEATURES_SET\r\n",
      "                        Set of features to include in the Feature Data.\r\n",
      "                        options = {\"mean\",\"std\",\"all\"}\r\n",
      "  -r REGION, --region REGION\r\n",
      "                        Input region of audios (from the beginning) for\r\n",
      "                        extracting local features. values = [0,1] (region = 1\r\n",
      "                        is equivalent of using the Global Features)\r\n",
      "  -c COMBINED, --combined COMBINED\r\n",
      "                        Input number of bins per octave in chroma vectors SET\r\n",
      "                        combined = 1 FOR PERFORMING CLASSIFICATION USING THE\r\n",
      "                        COMBINATION OF LOCAL AND GLOBAL FEATURES; combined = 0\r\n",
      "                        FOR USING ONLY THE LOCAL FEATURES (except the case\r\n",
      "                        region = 1, which ALREADY corresponds to global\r\n",
      "                        features)\r\n"
     ]
    }
   ],
   "source": [
    "!python3 DataFormatting.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 :  MACHINE LEARNING & AUTOMATIC AUDIO CLASSIFICATION\n",
    "\n",
    "### Input : \n",
    "#### PICKLE file : \n",
    "which is the output of the first notebook in the series (Feature Extraction).\n",
    "\n",
    "\n",
    "### Output :\n",
    "#### CSV file : \n",
    "The output CSV file that contains the FeatureData (X) and the class labels (Y), in appropriate format for the Machine Learning Pipeline.\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "\n",
    " - numBins(int) : number of bins in the HPCPvectors ([12,24,36,48])\n",
    " \n",
    " \n",
    " - region(float) : select the first 'x*100'% of the audio files for analysis. The classification is performed using the features that are extracted locally only from specified region of the songs.\n",
    " \n",
    "\n",
    " - combined (boolean) : if combined = 1, classification is performed using the combination of features that are extracted both locally and globally. if combined = 0, only locally obtained features are used for classification. The local region is specified by partSong. Default = 1.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing Local Features for the first0.3region of the audio files is COMPLETE \n",
      "\n",
      "Generating CSV file for the features meanLocal+stdGlobal is COMPLETE\n",
      "Generating CSV file for the features stdLocal+meanGlobal is COMPLETE\n",
      "Generating CSV file for the features meanLocal+meanGlobal is COMPLETE\n",
      "Generating CSV file for the features stdLocal+stdGlobal is COMPLETE\n"
     ]
    }
   ],
   "source": [
    "!python3 DataFormatting.py -i 'extractedFeatures_formakamtradition(48bins).pkl' -r 0.3 -c False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: Classification.py [-h] -i INPUT_FILENAME -c COMBINED -r REGION\r\n",
      "\r\n",
      "A tool for Chroma (HPCP) Feature Extraction using Essentia\r\n",
      "library.(CLASSIFICATION)\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -i INPUT_FILENAME, --input_filename INPUT_FILENAME\r\n",
      "                        Name of the input pickle file which is the output file\r\n",
      "                        of ChromaFeatureExtraction.py\r\n",
      "  -c COMBINED, --combined COMBINED\r\n",
      "                        Input number of bins per octave in chroma vectors SET\r\n",
      "                        combined = 1 FOR PERFORMING CLASSIFICATION USING THE\r\n",
      "                        COMBINATION OF LOCAL AND GLOBAL FEATURES; combined = 0\r\n",
      "                        FOR USING ONLY THE LOCAL FEATURES (except the case\r\n",
      "                        region = 1, which ALREADY corresponds to global\r\n",
      "                        features)\r\n",
      "  -r REGION, --region REGION\r\n",
      "                        Input region of audios (from the beginning) for\r\n",
      "                        extracting local features , SELECT THE REGION OF THE\r\n",
      "                        SONG TO BE ANALYZED LOCALLY. (region = 1 is equivalent\r\n",
      "                        of using the Global Features)\r\n"
     ]
    }
   ],
   "source": [
    "!python3  Classification.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataDir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7ba339da82f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m###CHOOSE THE CSV FILE WITH DESIRED FEATURE SET, TO INPUT FOR THE MACHINE LEARNING PIPELINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataDir' is not defined"
     ]
    }
   ],
   "source": [
    "###CHOOSE THE CSV FILE WITH DESIRED FEATURE SET, TO INPUT FOR THE MACHINE LEARNING PIPELINE\n",
    "import os\n",
    "os.listdir(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process might take a while (5-10 min) \n",
      " CROSS-VALIDATION & TRAINING \n",
      "{'Ussak', 'Rast', 'Nihavent', 'Beyati', 'Karcigar', 'Suzinak', 'Neva', 'Huseyni', 'Saba', 'Acemasiran', 'Hicaz', 'Kurdilihicazkar', 'Muhayyer', 'Acemkurdi', 'Sultaniyegah', 'Mahur', 'Hicazkar', 'Huzzam', 'Bestenigar', 'Segah'}\n",
      "Accuracy score for the Feature Set stdLocal+stdGlobal : \n",
      "F-measure (mean,std) --- FINAL\n",
      "0.76 0.0348500070129\n",
      "Accuracy (mean,std) FINAL\n",
      "0.77 0.0366196668472\n",
      "Confusion matrix, without normalization\n"
     ]
    }
   ],
   "source": [
    "!python3 Classification.py -i 'DataCSVforstage_48bins_stdLocal+stdGlobal.csv' -m makam -r 0.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
